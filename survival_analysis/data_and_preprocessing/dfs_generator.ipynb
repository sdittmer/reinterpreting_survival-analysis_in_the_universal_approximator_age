{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641edf7-05c1-4680-be35-2dd16ef4f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbimporter\n",
    "\n",
    "root = os.getcwd().split(\"survival_analysis\")[0]\n",
    "os.chdir(root + \"survival_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e7ecf-76cd-4684-93e9-ba076e5c9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lifelines.datasets as ds\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d31882",
   "metadata": {},
   "source": [
    "# Basic dataset functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05e7a5-3125-43ab-a996-6909c00e38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalization:\n",
    "\n",
    "    def __init__(self, df_train, df_valid, df_test):\n",
    "        self.df_train = df_train\n",
    "        self.df_valid = df_valid\n",
    "        self.df_test  = df_test\n",
    "        self.pre_normalization_checks(df_train, df_valid, df_test)\n",
    "\n",
    "\n",
    "    def pre_normalization_check(self, df):\n",
    "        assert \"duration\" in df.columns, df.columns\n",
    "        assert \"event_observed\" in df.columns, df.columns\n",
    "\n",
    "\n",
    "    def pre_normalization_checks(self, df_train, df_valid, df_test):\n",
    "        self.pre_normalization_check(df_train)\n",
    "        self.pre_normalization_check(df_valid)\n",
    "        self.pre_normalization_check(df_test)\n",
    "\n",
    "\n",
    "    def feature_names(self):\n",
    "        names = list(self.df_train.columns)\n",
    "        names.remove(\"duration\")\n",
    "        names.remove(\"event_observed\")\n",
    "        return names\n",
    "\n",
    "\n",
    "    def normalization_none(self):\n",
    "        return self.df_train.copy(), self.df_valid.copy(), self.df_test.copy()\n",
    "\n",
    "\n",
    "    def _normalize_mean_std(self, df, means, stds):\n",
    "        means = np.array(means[self.feature_names()])\n",
    "        stds = np.array(stds[self.feature_names()])\n",
    "\n",
    "        data = np.array(df[self.feature_names()].astype(float))\n",
    "        data = (data - means) / stds\n",
    "\n",
    "        feature_names = self.feature_names()\n",
    "        df.loc[:, feature_names] = data\n",
    "        return df\n",
    "\n",
    "\n",
    "    def normalize_mean_std(self):\n",
    "        df_train, df_valid, df_test = self.df_train.copy(), self.df_valid.copy(), self.df_test.copy()\n",
    "\n",
    "        means = df_train.mean().astype(float)\n",
    "        stds  = df_train.std().astype(float)\n",
    "\n",
    "        df_train = self._normalize_mean_std(df_train, means, stds)\n",
    "        df_valid = self._normalize_mean_std(df_valid, means, stds)\n",
    "        df_test  = self._normalize_mean_std(df_test,  means, stds)\n",
    "        return df_train, df_valid, df_test\n",
    "\n",
    "\n",
    "    def __call__(self, normalization):\n",
    "        if normalization is None:\n",
    "            return self.normalization_none()\n",
    "\n",
    "        if normalization == \"mean_std\":\n",
    "            return self.normalize_mean_std()\n",
    "\n",
    "        raise ValueError(f\"Bad argument: {normalization=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d424cf4-9b26-42c9-9296-279d18620c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaplanMeierDatasetAdjuster:\n",
    "\n",
    "    def __init__(self, df_generator):\n",
    "        self.df_generator = df_generator\n",
    "        self.kmf = self.get_KaplanMeier(df_generator)\n",
    "\n",
    "\n",
    "    def get_KaplanMeier(self, df_generator):\n",
    "        df_train = df_generator(horizon=None, normalization=None)[\"train\"]\n",
    "\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=df_train.duration, event_observed=df_train.event_observed, label=\"train\")\n",
    "        return kmf\n",
    "\n",
    "\n",
    "    def get_kmf_alive_fraction(self, horizon):\n",
    "        kmf_alive_fraction = self.kmf.survival_function_at_times(horizon).item()\n",
    "        return kmf_alive_fraction\n",
    "\n",
    "\n",
    "    def get_alive_in_train_dataset_fraction(self, horizon):\n",
    "        df_train = self.df_generator(horizon=horizon, normalization=None)[\"train\"]\n",
    "        alive_in_train_dataset_fraction = 1 - df_train.event_observed.mean()\n",
    "        return alive_in_train_dataset_fraction\n",
    "\n",
    "\n",
    "    def get_increase_factor_for_n_of_alive(self, horizon):\n",
    "        kmf_alive_fraction = self.get_kmf_alive_fraction(horizon)\n",
    "        alive_in_train_dataset_fraction = self.get_alive_in_train_dataset_fraction(horizon)\n",
    "\n",
    "        if alive_in_train_dataset_fraction == 0:\n",
    "            warnings.warn(f\"No one alive for {horizon=}.\")\n",
    "\n",
    "        if kmf_alive_fraction in {0, 1}:\n",
    "            assert alive_in_train_dataset_fraction in {0, 1}, f\"{kmf_alive_fraction=} but {alive_in_train_dataset_fraction=}\"\n",
    "            return 1\n",
    "\n",
    "        a = alive_in_train_dataset_fraction\n",
    "        r = kmf_alive_fraction\n",
    "        increase_factor_for_n_of_alive = r / (1 - r) * (1 - a) / a\n",
    "\n",
    "        return increase_factor_for_n_of_alive\n",
    "\n",
    "\n",
    "    def __call__(self, df, horizon):\n",
    "        df = df.copy()\n",
    "        increase_factor_for_n_of_alive = self.get_increase_factor_for_n_of_alive(horizon)\n",
    "        assert 0.9 < increase_factor_for_n_of_alive, f\"{increase_factor_for_n_of_alive=}\"\n",
    "\n",
    "        if increase_factor_for_n_of_alive == np.inf:\n",
    "            warnings.warn(f\"For {horizon=} we have {increase_factor_for_n_of_alive=}!\")\n",
    "            return df\n",
    "\n",
    "        df_alive = df[~df.event_observed]\n",
    "        fraction_to_add = np.clip(increase_factor_for_n_of_alive - 1, 0, np.inf)\n",
    "\n",
    "        df_alive = df_alive.sample(frac=fraction_to_add, replace=True, random_state=0).copy()\n",
    "\n",
    "        new_df = pd.concat([df, df_alive], ignore_index=True)\n",
    "        assert len(new_df) == len(df) + len(df_alive)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87848245-b872-4b23-a9e1-d5836672b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DfsGenerator:\n",
    "\n",
    "    def __init__(self, train_size=.5, seed=1, dfs_ssids=None, samples=None):\n",
    "        self.df_train, self.df_valid, self.df_test = self.get_basic_dfs(\n",
    "            train_size=train_size,\n",
    "            seed=seed,\n",
    "            dfs_ssids=dfs_ssids,\n",
    "            samples=samples\n",
    "        )\n",
    "\n",
    "        self.df_train, self.df_valid, self.df_test = self.remove_unwanted_columns(self.df_train, self.df_valid, self.df_test)\n",
    "\n",
    "        self.df_train.event_observed = self.df_train.event_observed.astype(bool)\n",
    "        self.df_valid.event_observed = self.df_valid.event_observed.astype(bool)\n",
    "        self.df_test.event_observed  = self.df_test.event_observed.astype(bool)\n",
    "\n",
    "        self._max_horizon = self._get_max_horizon()\n",
    "\n",
    "        self.check_that_column_names_are_valid()\n",
    "\n",
    "\n",
    "    def remove_unwanted_columns(self, df_train, df_valid, df_test):\n",
    "        df_train = df_train.drop(columns=self.get_columns_to_drop(df_train))\n",
    "        df_valid = df_valid.drop(columns=self.get_columns_to_drop(df_valid))\n",
    "        df_test  =  df_test.drop(columns=self.get_columns_to_drop(df_test))\n",
    "        return df_train, df_valid, df_test\n",
    "\n",
    "\n",
    "    def _get_max_horizon(self):\n",
    "        return self.df_train.duration.quantile(.9)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_horizon(self):\n",
    "        return self._max_horizon\n",
    "\n",
    "\n",
    "    def check_that_column_names_are_valid(self, df=None):\n",
    "        self.check_df(self.df_train)\n",
    "        self.check_df(self.df_valid)\n",
    "        self.check_df(self.df_test)\n",
    "\n",
    "        if df is not None:\n",
    "            if len(self.df_train.columns) == len(df.columns):\n",
    "                self.check_df(df)\n",
    "            else:\n",
    "                assert len(self.df_train.columns) - 2 == len(df.columns), \"len(df_train.columns) - 2 != len(df.columns)\"\n",
    "\n",
    "        assert (self.df_train.columns == self.df_valid.columns).all(), \"df_train.colums != df_valid.colums\"\n",
    "        assert (self.df_train.columns == self.df_test.columns).all(), \"df_train.colums != df_test.colums\"\n",
    "\n",
    "        if df is not None:\n",
    "            if len(self.df_train.columns) == len(df.columns):\n",
    "                assert (self.df_train.columns == df.columns).all(), \"df_train.colums != df.colums\"\n",
    "            else:\n",
    "                columns = list(self.df_train.columns)\n",
    "                columns.remove(\"duration\")\n",
    "                columns.remove(\"event_observed\")\n",
    "                assert (df.columns == columns).all(), \"df.columns != columns\"\n",
    "\n",
    "\n",
    "    def check_df(self, df):\n",
    "        assert \"duration\" in df.columns, df.columns\n",
    "        assert \"event_observed\" in df.columns, df.columns\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "    def get_columns_to_drop(self):\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "    def get_basic_dfs(self, train_size, seed, dfs_ssids=None, samples=None):\n",
    "        raise NotImplemented()\n",
    "\n",
    "\n",
    "    def modify_for_horizon(self, df, horizon, ε=1e-4):\n",
    "        df = df.copy()\n",
    "\n",
    "        if horizon is None:\n",
    "            return df\n",
    "\n",
    "        if horizon > self.max_horizon:\n",
    "            warnings.warn(f\"Careful, for {horizon=} little or no data is available. See, {self.max_horizon=}.\")\n",
    "\n",
    "        saw_death    = (df.duration <= horizon) &  df.event_observed\n",
    "        saw_drop_out = (df.duration <= horizon) & ~df.event_observed\n",
    "        rows_to_keep = ~saw_drop_out\n",
    "\n",
    "        df.event_observed = saw_death\n",
    "        df.duration = np.clip(df.duration, ε, horizon)\n",
    "        df = df[rows_to_keep]\n",
    "        return df\n",
    "\n",
    "\n",
    "    def adjust_alive_death_ratios_based_on_kaplan_maier(self, dfs, horizon):\n",
    "        adjuster = KaplanMeierDatasetAdjuster(self)\n",
    "\n",
    "        for name, df in dfs.items():\n",
    "            dfs[name] = adjuster(df, horizon)\n",
    "        return dfs\n",
    "\n",
    "\n",
    "    def _get_caching_name(self, horizon, normalization, adjust):\n",
    "        return f\"CACHE_{self.name}_{horizon}_{normalization}_{adjust}.pickle\"\n",
    "\n",
    "\n",
    "    def _cache(self, dfs, horizon, normalization, adjust):\n",
    "        name = self._get_caching_name(horizon, normalization, adjust)\n",
    "        pickle.dump(dfs, open(f\"data_and_preprocessing/cache/{name}\", 'wb'))\n",
    "\n",
    "\n",
    "    def _load_cached_version(self, horizon, normalization, adjust):\n",
    "        name = self._get_caching_name(horizon, normalization, adjust)\n",
    "\n",
    "        try:\n",
    "            dfs = pickle.load(open(f\"data_and_preprocessing/cache/{name}\", 'rb'))\n",
    "            return dfs\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_split(self, df, train_size, seed):\n",
    "        test_size = 1 - train_size\n",
    "        df_train, df_valid_and_test = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "        df_valid, df_test = train_test_split(df_valid_and_test, test_size=.5, random_state=seed)\n",
    "        return df_train, df_valid, df_test\n",
    "\n",
    "\n",
    "    def __call__(self, horizon, normalization=\"mean_std\", adjust=False):\n",
    "        dfs = self._load_cached_version(horizon, normalization, adjust)\n",
    "        if dfs is not None:\n",
    "            return dfs\n",
    "\n",
    "        data_normalization = DataNormalization(\n",
    "            self.df_train,\n",
    "            self.df_valid,\n",
    "            self.df_test\n",
    "        )\n",
    "        df_train, df_valid, df_test = data_normalization(normalization=normalization)\n",
    "\n",
    "        ε = 1e-4\n",
    "        df_train.duration = np.clip(df_train.duration, ε, np.inf)\n",
    "        df_valid.duration = np.clip(df_valid.duration, ε, np.inf)\n",
    "        df_test.duration  = np.clip(df_test.duration,  ε, np.inf)\n",
    "\n",
    "        df_train = self.modify_for_horizon(df=df_train, horizon=horizon, ε=ε)\n",
    "        df_valid = self.modify_for_horizon(df=df_valid, horizon=horizon, ε=ε)\n",
    "        df_test  = self.modify_for_horizon(df=df_test,  horizon=horizon, ε=ε)\n",
    "        dfs = {\"train\": df_train, \"valid\": df_valid, \"test\": df_test}\n",
    "\n",
    "        if adjust:\n",
    "            dfs = self.adjust_alive_death_ratios_based_on_kaplan_maier(dfs, horizon)\n",
    "\n",
    "        self._cache(dfs, horizon, normalization, adjust)\n",
    "\n",
    "        for df in dfs.values():\n",
    "            self.check_that_column_names_are_valid(df)\n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a91203-1f6a-48fc-ae51-85a267fad024",
   "metadata": {},
   "source": [
    "# Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663466d9-9bcf-4797-b224-15ecafcb07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gbsg2Generator(DfsGenerator):\n",
    "\n",
    "    def __init__(self, train_size=.5, seed=122, dfs_ssids=None, samples=None):\n",
    "        super().__init__(train_size=train_size, seed=seed, dfs_ssids=dfs_ssids, samples=samples)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"gbsg2\"\n",
    "\n",
    "\n",
    "    def get_columns_to_drop(self, df_train):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get_df_from_lifeline(self):\n",
    "        df = ds.load_gbsg2()\n",
    "        df = df.rename(columns={\"cens\": \"event_observed\", \"time\": \"duration\"})\n",
    "        df.event_observed = df.event_observed.astype(bool)\n",
    "\n",
    "        df.horTh = (df.horTh == \"yes\").astype(float)\n",
    "        df.menostat = (df.menostat == \"Post\").astype(float)\n",
    "\n",
    "        df.tgrade = df.tgrade.replace(\"I\", 1)\n",
    "        df.tgrade = df.tgrade.replace(\"II\", 2)\n",
    "        df.tgrade = df.tgrade.replace(\"III\", 3)\n",
    "        df.tgrade = df.tgrade.astype(float)\n",
    "\n",
    "        df.age = df.age.astype(float)\n",
    "        df.tsize = df.tsize.astype(float)\n",
    "        df.pnodes = df.pnodes.astype(float)\n",
    "        df.progrec = df.progrec.astype(float)\n",
    "        df.estrec = df.estrec.astype(float)\n",
    "        df.duration = df.duration / 100\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_basic_dfs(self, train_size, seed, dfs_ssids=None, samples=None):\n",
    "        df = self.get_df_from_lifeline()\n",
    "\n",
    "        df_train, df_valid, df_test = self.get_split(df=df, train_size=train_size, seed=seed)\n",
    "        return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c95b9-fc93-4628-b5ed-76129ad1bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurGenerator(DfsGenerator):\n",
    "\n",
    "    def __init__(self, train_size=.5, seed=114, dfs_ssids=None, samples=None):\n",
    "        super().__init__(train_size=train_size, seed=seed, dfs_ssids=dfs_ssids, samples=samples)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"recur\"\n",
    "\n",
    "\n",
    "    def get_columns_to_drop(self, df_train):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get_df_from_lifeline(self):\n",
    "        df = ds.load_recur()\n",
    "        df = df.rename(columns={\"CENSOR\": \"event_observed\", \"TIME1\": \"duration\"})\n",
    "        df.event_observed = df.event_observed.astype(bool)\n",
    "        df = df.drop(columns=[\"ID\"])\n",
    "        df.duration = (df.duration / 10)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_basic_dfs(self, train_size, seed, dfs_ssids=None, samples=None):\n",
    "        df = self.get_df_from_lifeline()\n",
    "\n",
    "        df_train, df_valid, df_test = self.get_split(df=df, train_size=train_size, seed=seed)\n",
    "        return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5286b94-0da6-4752-9d66-7fe958193eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LymphGenerator(DfsGenerator):\n",
    "\n",
    "    def __init__(self, train_size=.5, seed=139, dfs_ssids=None, samples=None):\n",
    "        super().__init__(train_size=train_size, seed=seed, dfs_ssids=dfs_ssids, samples=samples)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"lymph\"\n",
    "\n",
    "\n",
    "    def get_columns_to_drop(self, df_train):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get_df_from_lifeline(self):\n",
    "        df = ds.load_lymph_node()\n",
    "        df = df.reset_index()\n",
    "        df = df.drop(columns=[\"rectime\", \"diagdateb\", \"recdate\", \"deathdate\", \"censrec\", \"id\"])\n",
    "        df = df.rename(columns={\"censdead\": \"event_observed\", \"survtime\": \"duration\"})\n",
    "        df.event_observed = df.event_observed.astype(bool)\n",
    "        df.duration = (df.duration / 100)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_basic_dfs(self, train_size, seed, dfs_ssids=None, samples=None):\n",
    "        df = self.get_df_from_lifeline()\n",
    "\n",
    "        df_train, df_valid, df_test = self.get_split(df=df, train_size=train_size, seed=seed)\n",
    "        return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457acb18-2f5e-41bb-8596-e1c282bd2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaHousingGenerator(DfsGenerator):\n",
    "\n",
    "    def __init__(self, train_size=.80, seed=881, dfs_ssids=None, samples=None):\n",
    "        super().__init__(train_size=train_size, seed=seed, dfs_ssids=dfs_ssids, samples=samples)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"california\"\n",
    "\n",
    "\n",
    "    def get_columns_to_drop(self, df_train):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def _get_california_df(self):\n",
    "        xs, ys = fetch_california_housing(return_X_y=True)\n",
    "        df = pd.DataFrame(xs)\n",
    "        df[\"duration\"] = ys\n",
    "        df[\"event_observed\"] = True\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_basic_dfs(self, train_size, seed, dfs_ssids=None, samples=None):\n",
    "        df = self._get_california_df()\n",
    "\n",
    "        df_train, df_valid, df_test = self.get_split(df=df, train_size=train_size, seed=seed)\n",
    "        return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed491be1-e9e5-4670-b482-384f5e8d71da",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066773be-bdbe-465a-b190-f92ddad3bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle.dump(\n",
    "    Gbsg2Generator(),\n",
    "    open(\"data_and_preprocessing/df_generator_gbsg2.pickle\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e48b2-3274-463f-b66e-5e5187c4c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle.dump(\n",
    "    RecurGenerator(),\n",
    "    open(\"data_and_preprocessing/df_generator_recur.pickle\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01303dc-6592-4b69-86b6-3f4afb16db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle.dump(\n",
    "    LymphGenerator(),\n",
    "    open(\"data_and_preprocessing/df_generator_lymph.pickle\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a11fe4-df3c-4151-8ff4-4ee7782d1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle.dump(\n",
    "    CaliforniaHousingGenerator(),\n",
    "    open(\"data_and_preprocessing/df_generator_california.pickle\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796b6bc-c2e9-4838-88bf-c9c7c36b8184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

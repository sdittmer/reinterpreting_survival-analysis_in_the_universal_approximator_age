{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d103bbc2",
   "metadata": {},
   "source": [
    "# Monotonic increasing NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f067428-37ad-4360-a8da-eda0d7e717fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa739b",
   "metadata": {},
   "source": [
    "# MonotonicIncreasingNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004b312-1ec2-4f12-96bb-ad4993080a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftplusSimpleMonotonic(nn.Module):\n",
    "\n",
    "    def __init__(self, ω=1, β=0, output_size=64):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.w_ = nn.Parameter(torch.rand(1, output_size) * ω)\n",
    "        self.b  = nn.Parameter(torch.rand(1, output_size) * β)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def w(self):\n",
    "        self.w_.data.clamp_(0)\n",
    "        return self.w_\n",
    "\n",
    "\n",
    "    def transform(self, t):\n",
    "        assert t.shape == (len(t), 1), f\"{t.shape=}\"\n",
    "        return nn.functional.softplus(self.w * t - self.b)\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        y = self.transform(t)\n",
    "        assert t.shape == (t.shape[0], 1), f\"{t.shape=}\"\n",
    "        assert y.shape == (y.shape[0], self.output_size), f\"{y.shape=}\"\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edf3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicIncreasingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, z0_input_size, output_size, act, resblock=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.z0_input_size = z0_input_size\n",
    "        self.output_size = output_size\n",
    "        self.resblock = resblock\n",
    "\n",
    "        self.act = act\n",
    "        self.simple_monotone_fct = SoftplusSimpleMonotonic()\n",
    "\n",
    "        self.B = self._get_B(input_size, output_size)\n",
    "        self.G = self._get_G(input_size, self.simple_monotone_fct.output_size)\n",
    "        self.A = self._get_A(self.simple_monotone_fct.output_size, output_size)\n",
    "        self.L = nn.Linear(z0_input_size, output_size)\n",
    "\n",
    "        if resblock:\n",
    "            self.H = self._get_H(input_size, output_size)\n",
    "\n",
    "\n",
    "    def _get_B(self, input_size, output_size, scale_init_weight=0.2, scale_init_bias_B=-8.5):\n",
    "        B = nn.Linear(input_size, output_size)\n",
    "        B.weight.data = B.weight.data.abs() * scale_init_weight\n",
    "        B.bias.data = torch.rand_like(B.bias) * scale_init_bias_B\n",
    "        return B\n",
    "\n",
    "\n",
    "    def _get_G(self, input_size, output_size, scale_init_weight=0.2, scale_init_bias_G=10):\n",
    "        G = nn.Linear(input_size, output_size, bias=True)\n",
    "        G.weight.data = G.weight.data.abs() * scale_init_weight\n",
    "        G.bias.data = torch.rand_like(G.bias) * scale_init_bias_G\n",
    "        return G\n",
    "\n",
    "\n",
    "    def _get_A(self, input_size, output_size, scale_init_weight=0.2):\n",
    "        A = nn.Linear(input_size, output_size, bias=False)\n",
    "        A.weight.data = A.weight.data.abs() * scale_init_weight\n",
    "        return A\n",
    "\n",
    "\n",
    "    def _get_H(self, input_size, output_size, scale_init_weight=0.2):\n",
    "        H = nn.Linear(input_size, output_size)\n",
    "        H.weight.data = H.weight.data.abs() * scale_init_weight\n",
    "        return H\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _clamp_weights(self):\n",
    "        self.B.weight.data.clamp_(0)\n",
    "        self.A.weight.data.clamp_(0)\n",
    "        self.G.weight.data.clamp_(0)\n",
    "\n",
    "        if self.resblock:\n",
    "            self.H.weight.data.clamp_(0)\n",
    "\n",
    "\n",
    "    def forward(self, z, z0, t):\n",
    "        assert z.shape == (z.shape[0], self.input_size), f\"{z.shape=}, {z.shape=}, {(z.shape[0], self.input_size)=}\"\n",
    "        assert t.shape == (z.shape[0], 1)\n",
    "        assert torch.all(t >= 0)\n",
    "\n",
    "        self._clamp_weights()\n",
    "\n",
    "        γ = self.simple_monotone_fct(t)\n",
    "        Gz = self.G(z)\n",
    "        γGz = γ * nn.functional.softplus(Gz)\n",
    "        AγGz = self.A(γGz)\n",
    "\n",
    "        Bz = self.B(z)\n",
    "        Lz0 = self.L(z0)\n",
    "        z_new = self.act(Bz + AγGz + Lz0)\n",
    "\n",
    "        if self.resblock:\n",
    "            z_new = self.H(z) + z_new\n",
    "\n",
    "        assert z_new.shape == (z.shape[0], self.output_size)\n",
    "        return z_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bfc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicIncreasingVectorNet(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_sizes):\n",
    "        super().__init__()\n",
    "        self.sizes = latent_sizes\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(self.sizes) - 1):\n",
    "            is_last = (i == len(self.sizes) - 2)\n",
    "            act = nn.Identity() if is_last else nn.Tanh()\n",
    "\n",
    "            layer = MonotonicIncreasingLayer(\n",
    "                input_size=self.sizes[i],\n",
    "                z0_input_size=self.sizes[0],\n",
    "                output_size=self.sizes[i+1],\n",
    "                act=act,\n",
    "            )\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "\n",
    "    def _adjust_towards_zero(self, z, z0, t):\n",
    "        z = z - self(t=torch.zeros_like(t), z=z0, survival=False)\n",
    "\n",
    "        if np.isnan(torch.min(z).item()):\n",
    "            raise ValueError(\"Found a nan in one of MonotonicIncreasingVectorNet's activations.\")\n",
    "        assert torch.all(-1e-2 < z), f\"{torch.min(z)=}\"\n",
    "\n",
    "        z = torch.clamp(z, 0, np.inf)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def forward(self, t, z=None, survival=True):\n",
    "        assert t.shape == (t.shape[0], 1)\n",
    "        assert torch.all(t >= 0)\n",
    "\n",
    "        if z is None:\n",
    "            z = torch.zeros(t.shape[0], self.sizes[0], device=t.device)\n",
    "\n",
    "        z0 = z.clone()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            z = layer(z=z, z0=z0, t=t)\n",
    "\n",
    "        if survival:\n",
    "            z = self._adjust_towards_zero(z, z0, t)\n",
    "\n",
    "        assert z.shape == (t.shape[0], self.sizes[-1])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicIncreasingNet(MonotonicIncreasingVectorNet):\n",
    "\n",
    "    def __init__(self, latent_sizes=[32]*5):\n",
    "        super().__init__(latent_sizes=latent_sizes + [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d610e-22d0-48c8-8bb0-8a5a62811488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

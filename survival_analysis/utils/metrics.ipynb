{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e4e64",
   "metadata": {},
   "source": [
    "# Metric definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f577f0",
   "metadata": {},
   "source": [
    "## Basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72621648-c1bd-44e3-b1cf-dc37cc41fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMetrics:\n",
    "\n",
    "    def __init__(self, df, preds):\n",
    "        self.event_observed = copy.deepcopy(df.event_observed)\n",
    "        self.preds = copy.deepcopy(preds)\n",
    "\n",
    "\n",
    "    def get_n_rp(self):\n",
    "        n_rp = self.event_observed.sum()\n",
    "        assert 0 <= n_rp, f\"{n_rp=}\"\n",
    "        return n_rp\n",
    "\n",
    "\n",
    "    def get_n_rn(self):\n",
    "        n_rn = (~self.event_observed).sum()\n",
    "        assert 0 <= n_rn, f\"{n_rn=}\"\n",
    "        return n_rn\n",
    "\n",
    "\n",
    "    def get_n_tp(self):\n",
    "        n_tp = (self.preds * self.event_observed).sum()\n",
    "        assert 0 <= n_tp, f\"{n_tp=}\"\n",
    "        return n_tp\n",
    "\n",
    "\n",
    "    def get_n_fp(self):\n",
    "        n_fp = (self.preds * ~self.event_observed).sum()\n",
    "        assert 0 <= n_fp, f\"{n_fp=}\"\n",
    "        return n_fp\n",
    "\n",
    "\n",
    "    def get_n_tn(self):\n",
    "        n_tn = ((1 - self.preds) * ~self.event_observed).sum()\n",
    "        assert 0 <= n_tn, f\"{n_tn=}\"\n",
    "        return n_tn\n",
    "\n",
    "\n",
    "    def get_n_fn(self):\n",
    "        n_fn = ((1 - self.preds) * self.event_observed).sum()\n",
    "        assert 0 <= n_fn, f\"{n_fn=}\"\n",
    "        return n_fn\n",
    "\n",
    "\n",
    "    def get_n_pp(self):\n",
    "        n_pp = self.preds.sum()\n",
    "        assert 0 <= n_pp, f\"{n_pp=}\"\n",
    "        return n_pp\n",
    "\n",
    "\n",
    "    def get_n_np(self):\n",
    "        n_np = (1 - self.preds).sum()\n",
    "        assert 0 <= n_np, f\"{n_np=}\"\n",
    "        return n_np\n",
    "\n",
    "\n",
    "    def get_mse(self):\n",
    "        preds = copy.deepcopy(self.preds)\n",
    "        mse = ((self.event_observed - preds)**2).mean()\n",
    "        assert 0 <= mse <= 1, f\"{mse=}\"\n",
    "        return mse\n",
    "\n",
    "\n",
    "    def get_inverted_mse(self):\n",
    "        inverted_mse = 1 - self.get_mse()\n",
    "        return inverted_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7245b",
   "metadata": {},
   "source": [
    "## Advanced metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9f245-2e69-47e6-a5c8-1d9ad87109eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMetrics:\n",
    "\n",
    "    def __init__(self, stats_classifier, df_generator, part, adjust=True):\n",
    "        self.stats_classifier = stats_classifier\n",
    "        self.df_generator = df_generator\n",
    "        self.part = part\n",
    "\n",
    "        dfs = df_generator(horizon=stats_classifier.horizon, adjust=adjust)\n",
    "        self.df = dfs[part]\n",
    "\n",
    "        self.preds = self.stats_classifier.get_prediction(self.df)\n",
    "\n",
    "        self.basic_metrics = BasicMetrics(\n",
    "            df=self.df,\n",
    "            preds=self.preds,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _auroc_model(self):\n",
    "        if (~self.df.event_observed).all() or self.df.event_observed.all():\n",
    "            return 1.\n",
    "\n",
    "        probs = copy.deepcopy(self.preds)\n",
    "        auroc = roc_auc_score(self.df.event_observed, probs)\n",
    "\n",
    "        assert 0 <= auroc <= 1, f\"{auroc=}\"\n",
    "        return auroc\n",
    "\n",
    "\n",
    "    def _auprc_model(self):\n",
    "        probs = copy.deepcopy(self.preds)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "            precision, recall, _ = precision_recall_curve(self.df.event_observed, probs)\n",
    "\n",
    "        recall[np.isnan(recall)] = 0\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        assert 0 <= auprc <= 1 + 1e-5, f\"{auprc=}\"\n",
    "        return auprc\n",
    "\n",
    "\n",
    "    def _acc_model(self):\n",
    "        n_tp = self.basic_metrics.get_n_tp()\n",
    "        n_tn = self.basic_metrics.get_n_tn()\n",
    "\n",
    "        n_rp = self.basic_metrics.get_n_rp()\n",
    "        n_rn = self.basic_metrics.get_n_rn()\n",
    "\n",
    "        if n_rp + n_rn == 0:\n",
    "            return 0\n",
    "\n",
    "        acc = (n_tp + n_tn) / (n_rp + n_rn)\n",
    "\n",
    "        assert n_tp + n_tn <= n_rp + n_rn\n",
    "        assert 0 <= acc <= 1, f\"{acc=}\"\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def _tpr_model(self):\n",
    "        n_tp = self.basic_metrics.get_n_tp()\n",
    "\n",
    "        if n_tp == 0:\n",
    "            return 0\n",
    "\n",
    "        n_rp = self.basic_metrics.get_n_rp()\n",
    "        tpr = n_tp / (n_rp + 1e-9)\n",
    "\n",
    "        assert n_tp <= n_rp\n",
    "        assert 0 <= tpr <= 1, f\"{tpr=}\"\n",
    "        return tpr\n",
    "\n",
    "\n",
    "    def _tnr_model(self):\n",
    "        n_tn = self.basic_metrics.get_n_tn()\n",
    "\n",
    "        if n_tn == 0:\n",
    "            return 0\n",
    "\n",
    "        n_rn = self.basic_metrics.get_n_rn()\n",
    "        tnr = n_tn / (n_rn + 1e-9)\n",
    "\n",
    "        assert n_tn <= n_rn\n",
    "        assert 0 <= tnr <= 1, f\"{tnr=}\"\n",
    "        return tnr\n",
    "\n",
    "\n",
    "    def _baa_model(self):\n",
    "        tpr = self._tpr_model()\n",
    "        tnr = self._tnr_model()\n",
    "        return (tpr + tnr) / 2\n",
    "\n",
    "\n",
    "    def _you_model(self):\n",
    "        tpr = self._tpr_model()\n",
    "        tnr = self._tnr_model()\n",
    "        you = abs(tpr + tnr - 1)\n",
    "\n",
    "        assert 0 <= you <= 1, f\"{you=}\"\n",
    "        return you\n",
    "\n",
    "\n",
    "    def _pre_model(self):\n",
    "        n_tp = self.basic_metrics.get_n_tp()\n",
    "\n",
    "        if n_tp == 0:\n",
    "            return 0\n",
    "\n",
    "        n_pp = self.basic_metrics.get_n_pp()\n",
    "        pre = n_tp / (n_pp + 1e-9)\n",
    "\n",
    "        assert n_tp <= n_pp\n",
    "        assert 0 <= pre <= 1, f\"{pre=}\"\n",
    "        return pre\n",
    "\n",
    "\n",
    "    def _fβ_model(self, β):\n",
    "        pre = self._pre_model()\n",
    "        tpr = self._tpr_model()\n",
    "        fβ = (1 + β**2) * pre * tpr / (β**2 * pre + tpr + 1e-9)\n",
    "\n",
    "        assert 0 <= fβ <= 1, f\"{fβ=}\"\n",
    "        return fβ\n",
    "\n",
    "\n",
    "    def _metric_names(self):\n",
    "        metric_names = [\"F2\", \"Precision\", \"AUPRC\", \"Accuracy\", \"Balanced accuracy\", \"Youden\", \"Sensitivity\", \"F1\", \"F_0.5\", \"AUROC\", \"Specificity\", \"1 - MSE\"]\n",
    "        return metric_names\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        metrics = {\n",
    "            \"F2\": self._fβ_model(β=2),\n",
    "            \"Precision\": self._pre_model(),\n",
    "            \"AUPRC\": self._auprc_model(),\n",
    "            \"Accuracy\": self._acc_model(),\n",
    "            \"Balanced accuracy\": self._baa_model(),\n",
    "            \"Youden\": self._you_model(),\n",
    "            \"Sensitivity\": self._tpr_model(),\n",
    "            \"F1\": self._fβ_model(β=1),\n",
    "            \"F_0.5\": self._fβ_model(β=.5),\n",
    "            \"AUROC\": self._auroc_model(),\n",
    "            \"Specificity\": self._tnr_model(),\n",
    "            \"1 - MSE\": self.basic_metrics.get_inverted_mse(),\n",
    "        }\n",
    "\n",
    "        assert list(metrics.keys()) == self._metric_names(), \"Metrics returned and expected are out of sync.\"\n",
    "        return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
